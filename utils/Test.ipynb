{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.5 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.utils.splits import create_splits_scenes\n",
    "\n",
    "\n",
    "DATA_PATH = \"../data/v1.0-mini/\" #path to data stored\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=DATA_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "NUM_AGENTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'val', 'test', 'mini_train', 'mini_val', 'train_detect', 'train_track'])\n",
      "Number of samples in train set: 8\n"
     ]
    }
   ],
   "source": [
    "#         self.trainset = get_prediction_challenge_split(\"mini_train\", dataroot=data_path)\n",
    "helper = PredictHelper(nusc)\n",
    "\n",
    "#get all the scenes\n",
    "scenes = create_splits_scenes()\n",
    "print(scenes.keys())\n",
    "#get all the scenes in the trainset\n",
    "set_name = \"mini_train\"\n",
    "trainset = scenes[set_name] #List of scenes as part of training set\n",
    "prediction_scenes = json.load(open(DATA_PATH+\"maps/prediction_scenes.json\", \"r\")) #Dictionary containing list of instance and sample tokens for each scene\n",
    " \n",
    "print(\"Number of samples in train set: %d\" % (len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "dict_keys(['2e19253f3bac458191cf64ff2b22ba2b', 'd9a4df9e92804a44a039eb21e0dc1349'])\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "scene = trainset[idx]\n",
    "scene_tokens = prediction_scenes[scene] \n",
    "\n",
    "# print(scene_tokens)\n",
    "if len(scene_tokens) < 2:\n",
    "    print(\"Not enough agents in the scene\")\n",
    "    \n",
    "else:\n",
    "    #get the tokens in the scene: we will be using the instance tokens as that is the agent in the scene\n",
    "    tokens = [scene_tok.split(\"_\") for scene_tok in scene_tokens]\n",
    "    instance_tokens, sample_tokens = list(list(zip(*tokens))[0]), list(list(zip(*tokens))[1]) #List of instance tokens and sample tokens\n",
    "    \n",
    "    tokenCount = Counter(instance_tokens) #Dictionary containing count for number of samples per token\n",
    "    minCount = sorted(list(tokenCount.values()), reverse=True)[NUM_AGENTS-1] #used to find n agents with highest number of sample_tokens\n",
    "    \n",
    "    #Convert isntance and sample tokens to dict format\n",
    "    instance_sample_tokens = {}\n",
    "    for instance_token, sample_token in zip(instance_tokens, sample_tokens):\n",
    "        if tokenCount[instance_token] >= minCount:\n",
    "            try:\n",
    "                instance_sample_tokens[instance_token].append(sample_token)\n",
    "            except:\n",
    "                instance_sample_tokens[instance_token] = [sample_token]\n",
    "    \n",
    "    \n",
    "    print(len(instance_sample_tokens.keys()))\n",
    "    print(instance_sample_tokens.keys())\n",
    "#     print(instance_tokens)\n",
    "#     print(\"---------------\")\n",
    "#     print(sample_tokens)\n",
    "\n",
    "    # print(scenes[\"mini_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 5, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 23, 4 , 5]\n",
    "b = [2, 3, -4 , -5]\n",
    "a.sort(reverse=True)\n",
    "print(a)\n",
    "# for aa,bb in zip(a, b):\n",
    "#     print(aa, \"     \", bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
